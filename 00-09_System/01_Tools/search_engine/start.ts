#!/usr/bin/env bun
/**
 * Unified Startup Script
 * Starts both the embedding server (llama-server) and the API server
 */

import { spawn, type Subprocess } from "bun";

const LLAMA_SERVER = "/opt/homebrew/bin/llama-server";
const MODEL_PATH = "/Users/ronaldeddings/.lmstudio/models/Qwen/Qwen3-Embedding-8B-GGUF/Qwen3-Embedding-8B-Q8_0.gguf";
const EMBED_PORT = 8081;
const API_PORT = 3000;

// Track child processes for cleanup
let embedProcess: Subprocess | null = null;
let apiProcess: Subprocess | null = null;

async function checkPort(port: number): Promise<boolean> {
  try {
    const response = await fetch(`http://localhost:${port}/health`, {
      signal: AbortSignal.timeout(1000)
    });
    return response.ok;
  } catch {
    return false;
  }
}

async function waitForServer(port: number, name: string, maxWait = 60000): Promise<boolean> {
  const start = Date.now();
  process.stdout.write(`‚è≥ Waiting for ${name} on port ${port}`);

  while (Date.now() - start < maxWait) {
    if (await checkPort(port)) {
      console.log(` ‚úÖ`);
      return true;
    }
    process.stdout.write(".");
    await Bun.sleep(1000);
  }

  console.log(` ‚ùå timeout`);
  return false;
}

async function startEmbeddingServer(): Promise<Subprocess> {
  console.log("üöÄ Starting embedding server (llama-server)...");
  console.log(`   Model: ${MODEL_PATH.split('/').pop()}`);
  console.log(`   Port: ${EMBED_PORT}`);

  const proc = spawn({
    cmd: [
      LLAMA_SERVER,
      "-m", MODEL_PATH,
      "--embedding",
      "--pooling", "last",
      "--port", String(EMBED_PORT),
      "-np", "1",           // 1 parallel slot
      "-c", "8192",         // context size
      "-ngl", "99",         // GPU layers
      "-nocb"               // no continuous batching
    ],
    stdout: "pipe",
    stderr: "pipe",
  });

  // Log embedding server output in background
  (async () => {
    const reader = proc.stderr?.getReader();
    if (!reader) return;
    const decoder = new TextDecoder();
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      const text = decoder.decode(value);
      // Only log important messages
      if (text.includes("HTTP server") || text.includes("error") || text.includes("model loaded")) {
        console.log(`   [embed] ${text.trim()}`);
      }
    }
  })();

  return proc;
}

async function startAPIServer(): Promise<Subprocess> {
  console.log("\nüåê Starting API server...");
  console.log(`   Port: ${API_PORT}`);

  const proc = spawn({
    cmd: ["bun", "server.ts"],
    cwd: import.meta.dir,
    stdout: "inherit",
    stderr: "inherit",
  });

  return proc;
}

async function cleanup() {
  console.log("\n\nüõë Shutting down...");

  if (apiProcess) {
    console.log("   Stopping API server...");
    apiProcess.kill();
  }

  if (embedProcess) {
    console.log("   Stopping embedding server...");
    embedProcess.kill();
  }

  console.log("   Done. Goodbye!");
  process.exit(0);
}

// Handle shutdown signals
process.on("SIGINT", cleanup);
process.on("SIGTERM", cleanup);

async function main() {
  console.log("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó");
  console.log("‚ïë       VRAM Search Engine Startup       ‚ïë");
  console.log("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n");

  // Check if embedding server is already running
  if (await checkPort(EMBED_PORT)) {
    console.log(`‚úÖ Embedding server already running on port ${EMBED_PORT}`);
  } else {
    // Start embedding server
    embedProcess = await startEmbeddingServer();

    // Wait for embedding server to be ready
    const embedReady = await waitForServer(EMBED_PORT, "embedding server", 90000);
    if (!embedReady) {
      console.error("‚ùå Failed to start embedding server");
      console.error("   Check that the model file exists and llama-server is installed");
      process.exit(1);
    }
  }

  // Check if API server is already running
  if (await checkPort(API_PORT)) {
    console.log(`‚ö†Ô∏è  API server already running on port ${API_PORT}`);
    console.log("   Kill it first or use a different port");
    process.exit(1);
  }

  // Start API server
  apiProcess = await startAPIServer();

  // Wait for API server to be ready
  const apiReady = await waitForServer(API_PORT, "API server", 10000);
  if (!apiReady) {
    console.error("‚ùå Failed to start API server");
    cleanup();
    process.exit(1);
  }

  console.log("\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó");
  console.log("‚ïë          All Systems Running!          ‚ïë");
  console.log("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£");
  console.log(`‚ïë  üîç Search UI:  http://localhost:${API_PORT}    ‚ïë`);
  console.log(`‚ïë  üß† Embeddings: http://localhost:${EMBED_PORT}   ‚ïë`);
  console.log("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£");
  console.log("‚ïë  Press Ctrl+C to stop all services     ‚ïë");
  console.log("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n");

  // Keep the process running and wait for child processes
  await Promise.race([
    apiProcess.exited,
    embedProcess?.exited ?? new Promise(() => {}),
  ]);

  console.log("‚ö†Ô∏è  A server process exited unexpectedly");
  cleanup();
}

main().catch((err) => {
  console.error("Fatal error:", err);
  cleanup();
});
